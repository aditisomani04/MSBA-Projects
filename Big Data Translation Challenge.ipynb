{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Professor Name: Dr James J. Lee\n",
    "Student Name: Aditi Somani\n",
    "Student ID: 4181152\n",
    "Subject: BUAN 5315 02 23SQ Big Data Analysis\n",
    "Total Points: 200\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Movie Recommendation Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source: https://www.codementor.io/spark/tutorial/building-a-recommender-with-apache-spark-python-example-app-part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a SparkContext configured for local mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File download\n",
    "\n",
    "Small: 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users. Last updated 9/2018.\n",
    "\n",
    "Full: 27,000,000 ratings and 1,100,000 tag applications applied to 58,000 movies by 280,000 users. Includes tag genome data with 14 million relevance scores across 1,100 tags. Last updated 9/2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'\n",
    "small_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download location(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "datasets_path = os.path.join('/home/jovyan/work', 'Data Translation Challenge - Aditi Somani')\n",
    "\n",
    "complete_dataset_path = os.path.join(datasets_path, 'ml-latest.zip')\n",
    "small_dataset_path = os.path.join(datasets_path, 'ml-latest-small.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "small_f = urllib.request.urlretrieve (small_dataset_url, small_dataset_path)\n",
    "complete_f = urllib.request.urlretrieve (complete_dataset_url, complete_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(small_dataset_path, \"r\") as z:\n",
    "    z.extractall(datasets_path)\n",
    "\n",
    "with zipfile.ZipFile(complete_dataset_path, \"r\") as z:\n",
    "    z.extractall(datasets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and parsing datasets\n",
    "Now we are ready to read in each of the files and create an RDD consisting of parsed lines. \n",
    "\n",
    "Each line in the ratings dataset (ratings.csv) is formatted as: \n",
    "+ userId,movieId,rating,timestamp \n",
    "\n",
    "Each line in the movies (movies.csv) dataset is formatted as:\n",
    "+ movieId,title,genres \n",
    "\n",
    "The format of these files is uniform and simple, so we can use Python split() to parse their lines once they are loaded into RDDs. Parsing the movies and ratings files yields two RDDs: \n",
    "+ For each line in the ratings dataset, we create a tuple of (UserID, MovieID, Rating). We drop the timestamp because we do not need it for this recommender.\n",
    "+ For each line in the movies dataset, we create a tuple of (MovieID, Title). We drop the genres because we do not use them for this recommender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ratings_file = os.path.join(datasets_path, 'ml-latest-small', 'ratings.csv')\n",
    "\n",
    "small_ratings_raw_data = sc.textFile(small_ratings_file)\n",
    "small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '1', '4.0'), ('1', '3', '4.0'), ('1', '6', '4.0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_ratings_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'Toy Story (1995)'),\n",
       " ('2', 'Jumanji (1995)'),\n",
       " ('3', 'Grumpier Old Men (1995)')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_movies_file = os.path.join(datasets_path, 'ml-latest-small', 'movies.csv')\n",
    "\n",
    "small_movies_raw_data = sc.textFile(small_movies_file)\n",
    "small_movies_raw_data_header = small_movies_raw_data.take(1)[0]\n",
    "\n",
    "small_movies_data = small_movies_raw_data.filter(lambda line: line!=small_movies_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1])).cache()\n",
    "    \n",
    "small_movies_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting ALS parameters using the small dataset\n",
    "\n",
    "In order to determine the best ALS parameters, we will use the small dataset. We need first to split it into train, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source uses see=0L, which is the previous version of python (2.x)\n",
    "# 0L should be written as 0 from now on\n",
    "\n",
    "training_RDD, validation_RDD, test_RDD = small_ratings_data.randomSplit([6, 2, 2], seed=0)\n",
    "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 0.908078105265682\n",
      "For rank 8 the RMSE is 0.916462973348527\n",
      "For rank 12 the RMSE is 0.917665030756129\n",
      "The best model was trained with rank 4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "seed = 5\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1\n",
    "ranks = [4, 8, 12]\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "for rank in ranks:\n",
    "    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "    predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print ('For rank {} the RMSE is {}'.format(rank, error))\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "\n",
    "print ('The best model was trained with rank {}'.format(best_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((372, 1084), 3.42419871162954),\n",
       " ((4, 1084), 3.866749726695713),\n",
       " ((402, 1084), 3.4099577968422152)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 457), (5.0, 4.381060760461434)),\n",
       " ((1, 1025), (5.0, 4.705295366590298)),\n",
       " ((1, 1089), (5.0, 4.979982471805129))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_and_preds.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.9113780946334407\n"
     ]
    }
   ],
   "source": [
    "model = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "predictions = model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print('For testing data the RMSE is {}'.format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the complete dataset to build the final model\n",
    "\n",
    "We need first to split it into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27753444 recommendations in the complete dataset\n"
     ]
    }
   ],
   "source": [
    "# Load the complete dataset file\n",
    "complete_ratings_file = os.path.join(datasets_path, 'ml-latest', 'ratings.csv')\n",
    "complete_ratings_raw_data = sc.textFile(complete_ratings_file)\n",
    "complete_ratings_raw_data_header = complete_ratings_raw_data.take(1)[0]\n",
    "\n",
    "# Parse\n",
    "complete_ratings_data = complete_ratings_raw_data.filter(lambda line: line!=complete_ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()\n",
    "    \n",
    "print ('There are {} recommendations in the complete dataset'.format(complete_ratings_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=0)\n",
    "\n",
    "complete_model = ALS.train(training_RDD, best_rank, seed=seed, \n",
    "                           iterations=iterations, lambda_=regularization_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.8318265262101795\n"
     ]
    }
   ],
   "source": [
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "predictions = complete_model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "\n",
    "print ('For testing data the RMSE is {}'.format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make recommendations\n",
    "Although we aim at building an online movie recommender, now that we know how to have our recommender model ready, we can give it a try providing some movie recommendations. This will help us coding the recommending engine later on when building the web service, and will explain how to use the model in any other circumstances.\n",
    "\n",
    "When using collaborative filtering, getting recommendations is not as simple as predicting for the new entries using a previously generated model. Instead, we need to train again the model but including the new user preferences in order to compare them with other users in the dataset. That is, the recommender needs to be trained every time we have new user ratings (although a single model can be used by multiple users of course!). This makes the process expensive, and it is one of the reasons why scalability is a problem (and Spark a solution!). Once we have our model trained, we can reuse it to obtain top recomendations for a given user or an individual rating for a particular movie. These are less costly operations than training the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we want to do, is give recommendations of movies with a certain minimum number of ratings. For that, we need to count the number of ratings per movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 58098 movies in the complete dataset\n"
     ]
    }
   ],
   "source": [
    "complete_movies_file = os.path.join(datasets_path, 'ml-latest', 'movies.csv')\n",
    "complete_movies_raw_data = sc.textFile(complete_movies_file)\n",
    "complete_movies_raw_data_header = complete_movies_raw_data.take(1)[0]\n",
    "\n",
    "# Parse\n",
    "complete_movies_data = complete_movies_raw_data.filter(lambda line: line!=complete_movies_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2])).cache()\n",
    "\n",
    "complete_movies_titles = complete_movies_data.map(lambda x: (int(x[0]),x[1]))\n",
    "    \n",
    "print('There are {} movies in the complete dataset'.format(complete_movies_titles.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts_and_averages(ID_and_ratings_tuple):\n",
    "    nratings = len(ID_and_ratings_tuple[1])\n",
    "    return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)\n",
    "\n",
    "movie_ID_with_ratings_RDD = (complete_ratings_data.map(lambda x: (x[1], x[2])).groupByKey())\n",
    "movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)\n",
    "movie_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding new user ratings\n",
    "Now we need to rate some movies for the new user. We will put them in a new RDD and we will use the user ID 0, that is not assigned in the MovieLens dataset. Check the dataset movies file for ID to Title assignment (so you know what movies are you actually rating)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User 1: Aditi Somani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user ratings: [(0, 190863, 3), (0, 147246, 4), (0, 165723, 2), (0, 147300, 5), (0, 52938, 5), (0, 93923, 1), (0, 151459, 3), (0, 175559, 2), (0, 158236, 5), (0, 142382, 4)]\n"
     ]
    }
   ],
   "source": [
    "new_user_ID = 0\n",
    "\n",
    "# The format of each line is (userID, movieID, rating)\n",
    "new_user_ratings = [\n",
    "        (0,190863,3), # 3 Musketeers (2011)\n",
    "        (0,147246,4), # Adventures of Mowgli (1973)\n",
    "        (0,165723,2), # Ae Dil Hai Mushkil (2016)\n",
    "        (0,147300,5), # Adventures Of Sherlock Holmes And Dr. Watson: The Twentieth Century Approaches (1986)\n",
    "        (0,52938,5), # Adventures of Mark Twain, The (1986)\n",
    "        (0,93923,1), # Agent Vinod (2012)\n",
    "        (0,151459,3), # Airlift (2016)\n",
    "        (0,175559,2), # Aiyyaa (2012)\n",
    "        (0,158236,5), # Ali Baba and the Forty Thieves (1954)\n",
    "        (0,142382,4), # Agneepath (2012)\n",
    "    ]\n",
    "new_user_ratings_RDD = sc.parallelize(new_user_ratings)\n",
    "print ('New user ratings: {}'.format(new_user_ratings_RDD.take(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data_with_new_ratings_RDD = complete_ratings_data.union(new_user_ratings_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model trained in 150.922 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "new_ratings_model = ALS.train(complete_data_with_new_ratings_RDD, best_rank, seed=seed,\n",
    "                              iterations=iterations, lambda_=regularization_parameter)\n",
    "tt = time() - t0\n",
    "\n",
    "print ('New model trained in {} seconds'.format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gettings Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs\n",
    "# keep just those not on the ID list (thanks Lei Li for spotting the error!)\n",
    "new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))\n",
    "\n",
    "# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies\n",
    "new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6216,\n",
       "  ((5.046652081520669, 'Nowhere in Africa (Nirgendwo in Afrika) (2001)'),\n",
       "   717)),\n",
       " (124320, ((5.015150103764061, 'Once a Thief (1965)'), 1)),\n",
       " (83916, ((4.239779462274003, 'Blues in the Night (1941)'), 9))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)\n",
    "new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))\n",
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)\n",
    "new_user_recommendations_rating_title_and_count_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User 1 : Aditi Somani, Scenario 1 \n",
    "FULL dataset, filtering out movies with less than 25 ratings (meaning 25 or more ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 15 recommended movies (with more than 25 reviews):\n",
      "('\"Lonely Wife', 5.870108142013315, 43)\n",
      "('Music for One Apartment and Six Drummers (2001)', 5.864985506876503, 31)\n",
      "('\"Things I Like', 5.771641506766851, 30)\n",
      "('Cranford (2007)', 5.754259462579273, 35)\n",
      "('The Garden of Sinners - Chapter 5: Paradox Paradigm (2008)', 5.721277947682164, 27)\n",
      "(\"Won't You Be My Neighbor? (2018)\", 5.70748342393759, 83)\n",
      "('Slaying the Badger', 5.689480971449388, 25)\n",
      "('Alone in the Wilderness (2004)', 5.658184439605291, 343)\n",
      "('Mei and the Kittenbus (2002)', 5.637207192507796, 44)\n",
      "('My Love (2006)', 5.635059975005253, 32)\n",
      "('Hamlet (Gamlet) (1964)', 5.606352083856942, 37)\n",
      "('\"Civil War', 5.605288714379184, 431)\n",
      "('\"Crucified Lovers', 5.5999938654481465, 25)\n",
      "('Life (2009)', 5.595653871758609, 166)\n",
      "('Olive Kitteridge (2014)', 5.591718109650474, 211)\n"
     ]
    }
   ],
   "source": [
    "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=25).takeOrdered(15, key=lambda x: -x[1])\n",
    "\n",
    "print ('TOP 15 recommended movies (with more than 25 reviews):\\n{}'.format('\\n'.join(map(str, top_movies))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User 1 : Aditi Somani, Scenario 2 \n",
    "\n",
    "FULL dataset, filtering out movies with less than 100 ratings (meaning 100 or more ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 15 recommended movies (with more than 100 reviews):\n",
      "('Alone in the Wilderness (2004)', 5.658184439605291, 343)\n",
      "('\"Civil War', 5.605288714379184, 431)\n",
      "('Life (2009)', 5.595653871758609, 166)\n",
      "('Olive Kitteridge (2014)', 5.591718109650474, 211)\n",
      "('\"Decalogue', 5.579240000970557, 547)\n",
      "('\"Best of Youth', 5.554732763576486, 548)\n",
      "(\"Smiley's People (1982)\", 5.553740799105896, 116)\n",
      "('Blue Planet II (2017)', 5.552545805339198, 349)\n",
      "('56 Up (2012)', 5.546527359315515, 193)\n",
      "('Frozen Planet (2011)', 5.527124650868659, 402)\n",
      "('Planet Earth (2006)', 5.526898391082175, 1384)\n",
      "('Ikiru (1952)', 5.5156413672066416, 1551)\n",
      "('Casablanca (1942)', 5.509163187436604, 31095)\n",
      "('Promises (2001)', 5.501614878969943, 149)\n",
      "('Harakiri (Seppuku) (1962)', 5.498013350397949, 679)\n"
     ]
    }
   ],
   "source": [
    "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=100).takeOrdered(15, key=lambda x: -x[1])\n",
    "\n",
    "print ('TOP 15 recommended movies (with more than 100 reviews):\\n{}'.format('\\n'.join(map(str, top_movies))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User 2: Abhishek Somani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user ratings: [(0, 173175, 4), (0, 82906, 5), (0, 139148, 3), (0, 89848, 4), (0, 146256, 5), (0, 98956, 5), (0, 158408, 4), (0, 164600, 2), (0, 158697, 1), (0, 184551, 3)]\n"
     ]
    }
   ],
   "source": [
    "new_user_ID = 0\n",
    "\n",
    "# The format of each line is (userID, movieID, rating)\n",
    "new_user_ratings = [\n",
    "        (0,173175,4), # Badrinath Ki Dulhania (2017)\n",
    "        (0,82906,5), # Baghban (2003)\n",
    "        (0,139148,3), # Bajrangi Bhaijaan (2015)\n",
    "        (0,89848,4), # Badmaash Company (2010)\n",
    "        (0,146256,5), # Bade Miyan Chote Miyan (1998)\n",
    "        (0,98956,5), # Barfi! (2012)\n",
    "        (0,158408,4), # Ajab Prem Ki Ghazab Kahani (2009)\n",
    "        (0,164600,2), # Akira (2016)\n",
    "        (0,158697,1), # Aitraaz (2004)\n",
    "        (0,184551,3), # Aiyaary (2018)\n",
    "    ]\n",
    "new_user_ratings_RDD = sc.parallelize(new_user_ratings)\n",
    "print ('New user ratings: {}'.format(new_user_ratings_RDD.take(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data_with_new_ratings_RDD = complete_ratings_data.union(new_user_ratings_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model trained in 197.159 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "new_ratings_model = ALS.train(complete_data_with_new_ratings_RDD, best_rank, seed=seed,\n",
    "                              iterations=iterations, lambda_=regularization_parameter)\n",
    "tt = time() - t0\n",
    "\n",
    "print ('New model trained in {} seconds'.format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gettings Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs\n",
    "# keep just those not on the ID list (thanks Lei Li for spotting the error!)\n",
    "new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))\n",
    "\n",
    "# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies\n",
    "new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6216,\n",
       "  ((4.342085124281496, 'Nowhere in Africa (Nirgendwo in Afrika) (2001)'),\n",
       "   717)),\n",
       " (124320, ((4.178952815614785, 'Once a Thief (1965)'), 1)),\n",
       " (83916, ((3.8758578701660866, 'Blues in the Night (1941)'), 9))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)\n",
    "new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))\n",
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)\n",
    "new_user_recommendations_rating_title_and_count_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User 2: Abhishek Somani, Scenario 1\n",
    "\n",
    "FULL dataset, filtering out movies with less than 25 ratings (meaning 25 or more ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 15 recommended movies (with more than 25 reviews):\n",
      "(\"India's Daughter (2015)\", 6.33296756728441, 25)\n",
      "('Out in the Dark (2012)', 6.23684469759667, 27)\n",
      "('Always (2011)', 5.947683051392968, 28)\n",
      "('\"Lord of the Rings: The Return of the King', 5.639035487784753, 57378)\n",
      "('Bridegroom (2013)', 5.637766977192641, 36)\n",
      "('The Red Pill (2016)', 5.589525603543304, 46)\n",
      "('\"Lord of the Rings: The Fellowship of the Ring', 5.587376033278204, 61883)\n",
      "('\"Things I Like', 5.579380106463301, 30)\n",
      "('\"Lord of the Rings: The Two Towers', 5.5730645122969875, 56696)\n",
      "('N.H 10 (2015)', 5.557907634446291, 35)\n",
      "('Jimmy Carr: Live (2004)', 5.488937260082846, 28)\n",
      "('Udta Punjab (2016)', 5.4353618781713, 37)\n",
      "('Holding the Man (2015)', 5.429306257218158, 39)\n",
      "('Queen (2014)', 5.419449759710034, 82)\n",
      "('\"Misérables in Concert', 5.407820872248806, 43)\n"
     ]
    }
   ],
   "source": [
    "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=25).takeOrdered(15, key=lambda x: -x[1])\n",
    "\n",
    "print ('TOP 15 recommended movies (with more than 25 reviews):\\n{}'.format('\\n'.join(map(str, top_movies))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User 2: Abhishek Somani, Scenario 2\n",
    "\n",
    "FULL dataset, filtering out movies with less than 100 ratings (meaning 100 or more ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 15 recommended movies (with more than 100 reviews):\n",
      "('\"Lord of the Rings: The Return of the King', 5.639035487784753, 57378)\n",
      "('\"Lord of the Rings: The Fellowship of the Ring', 5.587376033278204, 61883)\n",
      "('\"Lord of the Rings: The Two Towers', 5.5730645122969875, 56696)\n",
      "('Prayers for Bobby (2009)', 5.198114519581836, 102)\n",
      "('The Blue Planet (2001)', 5.078366014461743, 421)\n",
      "('Blue Planet II (2017)', 5.07280128557607, 349)\n",
      "('Doctor Who: The Husbands of River Song (2015)', 5.068229288321994, 239)\n",
      "('Frozen Planet (2011)', 5.058000585103578, 402)\n",
      "('Gangs of Wasseypur (2012)', 5.05719203931173, 167)\n",
      "('Doctor Who: The Time of the Doctor (2013)', 5.043155067985097, 394)\n",
      "('Life (2009)', 5.031069417476978, 166)\n",
      "('Doctor Who: Last Christmas (2014)', 5.021954923432752, 228)\n",
      "('Harry Potter and the Deathly Hallows: Part 2 (2011)', 4.995154400492718, 13262)\n",
      "('\"Swades: We', 4.985371442155568, 163)\n",
      "('Voices from the List (2004)', 4.983503029912285, 1800)\n"
     ]
    }
   ],
   "source": [
    "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=100).takeOrdered(15, key=lambda x: -x[1])\n",
    "\n",
    "print ('TOP 15 recommended movies (with more than 100 reviews):\\n{}'.format('\\n'.join(map(str, top_movies))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the results  and Insights/Foresights on both scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both the users in both the scenarios work well and generate top 15 recommendations based on the given criteria. The below are few insights from the above scenarios: \n",
    "\n",
    "1. The collaborative recommendation engine successfully generated top movie recommendations for both scenarios and users. And most of the recommendations are very aptly generated.\n",
    "\n",
    "2. Filtering out movies with less than 25 ratings resulted in a larger set of recommended movies compared to filtering out movies with less than 100 ratings. This suggests that a broader range of movies can be recommended when considering movies with a minimum of 25 ratings.\n",
    "\n",
    "3. The recommendations tailored to each user's ratings were able to provide personalized suggestions based on their preferences, indicating that the recommendation engine is effective in understanding individual customer preferences.\n",
    "\n",
    "4. The Pumpkinmeter score, derived from collaborative filtering, has the potential to enhance customer engagement by providing relevant movie recommendations. This can lead to increased customer satisfaction and likelihood of staying with Ripe Pumpkins' services over competitors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
